{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 23. Introdução ao LangGraph: Fluxos Cíclicos\n",
                "\n",
                "Até agora, nossas Chains eram lineares (DAGs). Mas e se precisarmos de um **loop**? Ex: \"Escreva um código, teste. Se der erro, corrija e teste de novo\". Para isso serve o `LangGraph`.\n",
                "\n",
                "**Objetivos:**\n",
                "- Entender `StateGraph`, `Nodes` e `Edges`.\n",
                "- Criar um fluxo com condicional (Router)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -qU langchain langchain-openai langchain-community langgraph"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from google.colab import userdata\n",
                "import getpass\n",
                "\n",
                "try:\n",
                "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
                "except:\n",
                "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Digite sua OpenAI API Key: \")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Definindo o Estado (State)\n",
                "\n",
                "O Estado é um dicionário (ou objeto Pydantic) compartilhado entre todos os nós do grafo."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import TypedDict\n",
                "\n",
                "class AgenteState(TypedDict):\n",
                "    pergunta: str\n",
                "    resposta: str\n",
                "    revisoes: int"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Definindo os Nós (Nodes)\n",
                "\n",
                "São funções python simples que recebem o estado e retornam uma atualização para ele."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_openai import ChatOpenAI\n",
                "from langchain_core.prompts import ChatPromptTemplate\n",
                "from langchain_core.output_parsers import StrOutputParser\n",
                "\n",
                "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
                "\n",
                "def gerador(state: AgenteState):\n",
                "    print(f\"--- GERANDO (Tentativa {state['revisoes']}) ---\")\n",
                "    prompt = ChatPromptTemplate.from_template(\"Responda a pergunta de forma breve: {pergunta}\")\n",
                "    chain = prompt | llm | StrOutputParser()\n",
                "    res = chain.invoke({\"pergunta\": state['pergunta']})\n",
                "    return {\"resposta\": res, \"revisoes\": state['revisoes'] + 1}\n",
                "\n",
                "def critico(state: AgenteState):\n",
                "    print(\"--- CRITICANDO ---\")\n",
                "    # Simulação: se a resposta tiver menos de 20 chars, achamos ruim\n",
                "    # Se já revisou 3 vezes, aceitamos de qualquer jeito para evitar loop infinito\n",
                "    if len(state['resposta']) < 20 and state['revisoes'] < 3:\n",
                "        return {\"pergunta\": state['pergunta'] + \" (Seja mais detalhado!)\"}\n",
                "    return {} # Nenhuma mudança"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Definindo a Lógica Condicional (Edges)\n",
                "\n",
                "Decide se volta para o gerador ou termina."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langgraph.graph import END\n",
                "\n",
                "def router(state: AgenteState):\n",
                "    # Se a resposta for curta e tivermos revisões sobrando, volta pro gerador\n",
                "    if len(state['resposta']) < 20 and state['revisoes'] < 3:\n",
                "        return \"gerador\"\n",
                "    return END"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Montando o Grafo\n",
                "\n",
                "Adicionamos nós e arestas."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langgraph.graph import StateGraph, START\n",
                "\n",
                "workflow = StateGraph(AgenteState)\n",
                "\n",
                "workflow.add_node(\"gerador\", gerador)\n",
                "workflow.add_node(\"critico\", critico)\n",
                "\n",
                "# Fluxo: Start -> Gerador -> Critico -> Router -> (Gerador ou Fim)\n",
                "workflow.add_edge(START, \"gerador\")\n",
                "workflow.add_edge(\"gerador\", \"critico\")\n",
                "\n",
                "workflow.add_conditional_edges(\n",
                "    \"critico\",\n",
                "    router,\n",
                "    {\n",
                "        \"gerador\": \"gerador\",\n",
                "        END: END\n",
                "    }\n",
                ")\n",
                "\n",
                "app = workflow.compile()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Executando\n",
                "\n",
                "Vamos fazer uma pergunta que exige resposta curta para ver se ele entra no loop."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "inputs = {\"pergunta\": \"Quem descobriu o Brasil?\", \"revisoes\": 0, \"resposta\": \"\"}\n",
                "output = app.invoke(inputs)\n",
                "\n",
                "print(\"\\n=== FINAL ===\")\n",
                "print(output['resposta'])\n",
                "print(f\"Total revisões: {output['revisoes']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusão\n",
                "\n",
                "Criamos um sistema que se auto-corrige! O `LangGraph` permite criar fluxos de trabalho resilientes onde o agente pode tentar novamente se falhar."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}