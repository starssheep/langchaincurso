{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 07. RAG Parte 3: RetrievalQA\n",
                "\n",
                "Agora vamos juntar as peças: Docs -> Split -> Vector Store -> Retriever -> LLM -> Resposta.\n",
                "\n",
                "**Objetivos:**\n",
                "- Criar uma `create_retrieval_chain` para responder perguntas baseadas nos documentos."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -qU langchain langchain-openai langchain-community faiss-cpu python-dotenv"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from google.colab import userdata\n",
                "import getpass\n",
                "\n",
                "try:\n",
                "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
                "except:\n",
                "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Digite sua OpenAI API Key: \")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup Rápido (Load, Split, Index)\n",
                "\n",
                "Recriando o índice para usar aqui."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_community.document_loaders import WebBaseLoader\n",
                "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
                "from langchain_openai import OpenAIEmbeddings\n",
                "from langchain_community.vectorstores import FAISS\n",
                "\n",
                "# 1. Load\n",
                "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
                "docs = loader.load()\n",
                "\n",
                "# 2. Split\n",
                "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
                "splits = text_splitter.split_documents(docs)\n",
                "\n",
                "# 3. Index\n",
                "vectorstore = FAISS.from_documents(splits, OpenAIEmbeddings())\n",
                "retriever = vectorstore.as_retriever()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Criando a Chain de RAG\n",
                "\n",
                "Usaremos `create_stuff_documents_chain` (que insere os docs no prompt) e `create_retrieval_chain` (que gerencia a busca)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.chains import create_retrieval_chain\n",
                "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
                "from langchain_core.prompts import ChatPromptTemplate\n",
                "from langchain_openai import ChatOpenAI\n",
                "\n",
                "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
                "\n",
                "# Prompt do sistema que receberá o contexto\n",
                "system_prompt = (\n",
                "    \"Você é um assistente para tarefas de perguntas e respostas. \"\n",
                "    \"Use os seguintes pedaços de contexto recuperado para responder à pergunta. \"\n",
                "    \"Se você não souber a resposta, diga que não sabe. \"\n",
                "    \"Use no máximo três frases e mantenha a resposta concisa.\"\n",
                "    \"\\n\\n\"\n",
                "    \"{context}\"\n",
                ")\n",
                "\n",
                "prompt = ChatPromptTemplate.from_messages(\n",
                "    [\n",
                "        (\"system\", system_prompt),\n",
                "        (\"human\", \"{input}\"),\n",
                "    ]\n",
                ")\n",
                "\n",
                "# Chain que combina documentos no prompt\n",
                "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
                "\n",
                "# Chain final que recupera docs e passa para a chain acima\n",
                "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Testando\n",
                "\n",
                "Vamos fazer uma pergunta sobre o artigo."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "response = rag_chain.invoke({\"input\": \"What is Task Decomposition?\"})\n",
                "\n",
                "print(response[\"answer\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Inspecionando a Fonte\n",
                "\n",
                "Podemos ver quais documentos foram usados para gerar a resposta."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for i, doc in enumerate(response[\"context\"]):\n",
                "    print(f\"Documento {i}: {doc.page_content[:100]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusão\n",
                "\n",
                "Temos um sistema de RAG funcional! Ele recupera informação relevante e responde de forma fundamentada.\n",
                "\n",
                "No próximo notebook, vamos sair do padrão \"pergunta-resposta\" e entrar no mundo dos **Agentes**, que podem usar ferramentas para agir."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}