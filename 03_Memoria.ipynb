{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 03. Memória\n",
                "\n",
                "Os modelos de linguagem são \"stateless\" (sem estado), ou seja, eles não lembram da conversa passada por padrão. Para criar chatbots, precisamos gerenciar o histórico da conversa e passá-lo a cada nova interação. O LangChain facilita isso.\n",
                "\n",
                "**Objetivos:**\n",
                "- Entender como funciona a memória no LCEL.\n",
                "- Usar `RunnableWithMessageHistory` para gerenciar histórico automaticamente."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -qU langchain langchain-openai langchain-community python-dotenv"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from google.colab import userdata\n",
                "import getpass\n",
                "\n",
                "try:\n",
                "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
                "except:\n",
                "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Digite sua OpenAI API Key: \")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_openai import ChatOpenAI\n",
                "from langchain_core.prompts import ChatPromptTemplate\n",
                "from langchain_core.output_parsers import StrOutputParser\n",
                "\n",
                "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. O Problema da Falta de Memória\n",
                "\n",
                "Vamos ver como o modelo se comporta sem memória."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "chain = ChatPromptTemplate.from_template(\"{input}\") | llm | StrOutputParser()\n",
                "\n",
                "# Primeira interação\n",
                "print(chain.invoke({\"input\": \"Oi, meu nome é Nauber.\"}))\n",
                "\n",
                "# Segunda interação\n",
                "print(chain.invoke({\"input\": \"Qual é o meu nome?\"}))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Ele provavlemente dirá que não sabe, pois cada chamada é independente."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Adicionando Histórico com `RunnableWithMessageHistory`\n",
                "\n",
                "Essa é a forma recomendada no LCEL moderno. Precisamos de uma classe para armazenar o histórico (aqui usaremos `ChatMessageHistory` em memória, mas poderia ser num banco de dados)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_community.chat_message_histories import ChatMessageHistory\n",
                "from langchain_core.chat_history import BaseChatMessageHistory\n",
                "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
                "\n",
                "# Dicionário para guardar os históricos de diferentes sessões (session_ids)\n",
                "store = {}\n",
                "\n",
                "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
                "    if session_id not in store:\n",
                "        store[session_id] = ChatMessageHistory()\n",
                "    return store[session_id]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Agora criamos o prompt aceitando um `MessagesPlaceholder` para injetar o histórico."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_core.prompts import MessagesPlaceholder\n",
                "\n",
                "prompt_with_history = ChatPromptTemplate.from_messages([\n",
                "    (\"system\", \"Você é um assistente prestativo.\"),\n",
                "    MessagesPlaceholder(variable_name=\"history\"),\n",
                "    (\"human\", \"{input}\")\n",
                "])\n",
                "\n",
                "runnable = prompt_with_history | llm | StrOutputParser()\n",
                "\n",
                "# Envolvemos a chain original com a capacidade de histórico\n",
                "with_message_history = RunnableWithMessageHistory(\n",
                "    runnable,\n",
                "    get_session_history,\n",
                "    input_messages_key=\"input\",\n",
                "    history_messages_key=\"history\"\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Testando a Memória\n",
                "\n",
                "Agora vamos conversar passando um `session_id`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configurando o ID da sessão\n",
                "config = {\"configurable\": {\"session_id\": \"sessao_do_nauber\"}}\n",
                "\n",
                "response1 = with_message_history.invoke(\n",
                "    {\"input\": \"Oi, meu nome é Nauber.\"}, \n",
                "    config=config\n",
                ")\n",
                "print(f\"Resposta 1: {response1}\")\n",
                "\n",
                "response2 = with_message_history.invoke(\n",
                "    {\"input\": \"Qual é o meu nome?\"}, \n",
                "    config=config\n",
                ")\n",
                "print(f\"Resposta 2: {response2}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Chats Diferentes (Session IDs)\n",
                "\n",
                "Se mudarmos o `session_id`, ele não lembrará."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "config_novo = {\"configurable\": {\"session_id\": \"sessao_nova\"}}\n",
                "\n",
                "response3 = with_message_history.invoke(\n",
                "    {\"input\": \"Qual é o meu nome?\"}, \n",
                "    config=config_novo\n",
                ")\n",
                "print(f\"Resposta 3 (Sessão Nova): {response3}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusão\n",
                "\n",
                "Neste notebook, aprendemos a manter o estado da conversa usando `RunnableWithMessageHistory` e `ChatMessageHistory`.\n",
                "\n",
                "No próximo notebook, vamos explorar **Chains** mais complexas."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}