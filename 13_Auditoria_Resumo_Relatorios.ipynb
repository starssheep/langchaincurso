{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Auditoria: Resumo de Relatórios Longos\n",
    "\n",
    "Relatórios de auditoria podem ter centenas de páginas. Para a alta gestão, precisamos extrair apenas os \"Pontos de Atenção\" e \"Recomendações\". Quando o texto é maior que a janela de contexto do LLM, usamos técnicas como **Map-Reduce**.\n",
    "\n",
    "**Objetivo:** Resumir um texto longo identificando principais riscos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "!pip install -qU langchain langchain-openai langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "except ImportError:\n",
    "    userdata = None\n",
    "import getpass\n",
    "\n",
    "try:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "except:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Digite sua OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregando Texto Longo\n",
    "\n",
    "Vamos simular um relatório longo (repetindo texto) para forçar o uso da chain de resumo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_base = \"\"\"\n",
    "RELATÓRIO DE AUDITORIA INTERNA - ÁREA DE COMPRAS\n",
    "\n",
    "1. INTRODUÇÃO\n",
    "O objetivo desta auditoria foi avaliar os controles internos do ciclo de compras.\n",
    "\n",
    "2. ACHADO: FALTA DE TRÊS COTAÇÕES\n",
    "Identificamos que em 40% dos processos de compra acima de R$ 10.000, não houve a realização de três cotações conforme norma interna. Isso gera risco de sobrepreço.\n",
    "Recomendação: Implementar trava no sistema SAP impedindo pedido de compra sem anexos de cotação.\n",
    "\n",
    "3. ACHADO: APROVAÇÃO POR ALÇADA INCORRETA\n",
    "O Diretor Financeiro aprovou compras de TI que deveriam ser aprovadas pelo CTO. Risco: Aquisição de tecnologia incompatível.\n",
    "Recomendação: Revisar fluxo de workflow de aprovação.\n",
    "\n",
    "4. ACHADO: CADASTRO DE FORNECEDORES\n",
    "Fornecedores cadastrados sem documentação de Compliance. Risco: Contratação de empresas idôneas.\n",
    "Recomendação: Bloquear pagamentos a fornecedores com cadastro incompleto.\n",
    "\"\"\"\n",
    "\n",
    "# Multiplicando para ficar \"longo\" (simulação)\n",
    "docs_texto = [texto_base] * 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparando Documentos\n",
    "\n",
    "Transformando strings em objetos `Document`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "docs = [Document(page_content=t) for t in docs_texto]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Criando a Chain de Summarization (Map-Reduce)\n",
    "\n",
    "O LangChain possuía `load_summarize_chain`, mas em LCEL moderno construímos manualmente ou usamos a chain pronta de `stuff` se couber no contexto. Como modelos modernos (GPT-4-Turbo, Gemini 1.5) têm contextos gigantes (128k+ tokens), muitas vezes não precisamos mais de Map-Reduce complexo. Vamos usar a abordagem `Stuff` (colocar tudo no prompt) que é mais comum hoje, mas estruturando o resumo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Você é um assistente executivo.\n",
    "    \n",
    "    Resuma os seguintes relatórios de auditoria em uma lista de bullet points contendo apenas os ACHADOS e as RECOMENDAÇÕES principais.\n",
    "    Ignore textos introdutórios.\n",
    "    \n",
    "    RELATÓRIOS:\n",
    "    {context}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "resumo = chain.invoke({\"context\": docs})\n",
    "print(resumo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "Com poucas linhas, consolidamos informações repetitivas ou extensas em um sumário executivo direto ao ponto."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
