{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Prompt Templates e Output Parsers\n",
    "\n",
    "Agora que sabemos chamar os modelos, precisamos estruturar melhor nossos prompts e processar as respostas. Usaremos **Prompt Templates** e **Output Parsers**, e introduziremos a sintaxe LCEL (LangChain Expression Language).\n",
    "\n",
    "**Objetivos:**\n",
    "- Criar templates de prompt dinâmicos.\n",
    "- Usar parsers para limpar a saída.\n",
    "- Criar nossa primeira Chain usando o operador `|`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "!pip install -qU langchain langchain-openai langchain-community python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "except ImportError:\n",
    "    userdata = None\n",
    "import getpass\n",
    "\n",
    "try:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "except:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Digite sua OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prompt Templates\n",
    "\n",
    "Em vez de concatenar strings manualmente (`\"Traduza \" + texto + \" para inglês\"`), usamos `PromptTemplate`. Isso ajuda a organizar variáveis e permite reutilização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Criando um template a partir de mensagens\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Você é um tradutor profissional. Traduza o texto a seguir para {idioma}.\"),\n",
    "    (\"user\", \"{texto}\")\n",
    "])\n",
    "\n",
    "# Podemos ver como fica o prompt formatado\n",
    "prompt_val = prompt_template.invoke({\"idioma\": \"Francês\", \"texto\": \"O gato está na mesa.\"})\n",
    "print(prompt_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Output Parsers\n",
    "\n",
    "A resposta do modelo é um objeto `AIMessage`. Frequentemente queremos apenas o texto (string). O `StrOutputParser` extrai o conteúdo da mensagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LCEL: LangChain Expression Language\n",
    "\n",
    "É aqui que a mágica acontece. O LangChain moderno usa o operador \"pipe\" (`|`) para conectar componentes.\n",
    "\n",
    "Fluxo: `Prompt -> Modelo -> Parser`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a chain\n",
    "chain = prompt_template | llm | parser\n",
    "\n",
    "# Executando a chain\n",
    "# Passamos um dicionário com as variáveis definidas no prompt_template\n",
    "resultado = chain.invoke({\"idioma\": \"Espanhol\", \"texto\": \"Eu gosto de programar em Python.\"})\n",
    "\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exemplo Prático: Gerador de Nomes de Empresas\n",
    "\n",
    "Vamos criar uma chain que sugere nomes de empresas com base em um produto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Você é um consultor de branding criativo.\"),\n",
    "    (\"user\", \"Sugira 3 nomes criativos para uma empresa que fabrica {produto}.\")\n",
    "])\n",
    "\n",
    "name_chain = name_prompt | llm | parser\n",
    "\n",
    "print(name_chain.invoke({\"produto\": \"tênis de corrida feitos de material reciclado\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "Neste notebook, vimos como:\n",
    "1. Criar prompts com variáveis usando `ChatPromptTemplate`.\n",
    "2. Limpar a saída (extrair texto) usando `StrOutputParser`.\n",
    "3. Encadeá-los usando o pipe `|` (LCEL).\n",
    "\n",
    "No próximo notebook, aprenderemos a adicionar **Memória** às nossas conversas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
