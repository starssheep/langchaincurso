{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 04. Chains e Sequências\n",
                "\n",
                "Já vimos chains simples com `|`. Agora vamos criar fluxos mais complexos, onde a saída de uma chain serve de entrada para outra, e execução paralela.\n",
                "\n",
                "**Objetivos:**\n",
                "- Criar chains sequenciais.\n",
                "- Usar `RunnableParallel` para executar tarefas ao mesmo tempo."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -qU langchain langchain-openai langchain-community python-dotenv"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from google.colab import userdata\n",
                "import getpass\n",
                "\n",
                "try:\n",
                "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
                "except:\n",
                "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Digite sua OpenAI API Key: \")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_openai import ChatOpenAI\n",
                "from langchain_core.prompts import ChatPromptTemplate\n",
                "from langchain_core.output_parsers import StrOutputParser\n",
                "\n",
                "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Chain Sequencial\n",
                "\n",
                "Imagine que queremos:\n",
                "1. Gerar uma sinopse de um filme dado o título.\n",
                "2. Escrever uma crítica baseada nessa sinopse de filme.\n",
                "\n",
                "Isso são duas chains conectadas."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Chain 1: Sinopse\n",
                "prompt_sinopse = ChatPromptTemplate.from_template(\"Escreva uma sinopse breve para um filme chamado {titulo}.\")\n",
                "chain_sinopse = prompt_sinopse | llm | StrOutputParser()\n",
                "\n",
                "# Chain 2: Crítica\n",
                "# Nota: o input dessa chain será a sinopse gerada anteriormente\n",
                "prompt_critica = ChatPromptTemplate.from_template(\"Escreva uma crítica de cinema para a seguinte sinopse: {sinopse}.\")\n",
                "chain_critica = prompt_critica | llm | StrOutputParser()\n",
                "\n",
                "# Encadeando tudo\n",
                "# Usamos um dicionário lambda ou chain pura se o input bater\n",
                "# Aqui, a chain_sinopse retorna uma string. Precisamos passá-la como 'sinopse' para a chain_critica.\n",
                "from langchain_core.runnables import RunnablePassthrough\n",
                "\n",
                "chain_completa = (\n",
                "    {\"sinopse\": chain_sinopse} \n",
                "    | chain_critica\n",
                ")\n",
                "\n",
                "print(chain_completa.invoke({\"titulo\": \"As Aventuras do Programador Python\"}))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Execução Paralela (`RunnableParallel`)\n",
                "\n",
                "Às vezes queremos rodar duas coisas ao mesmo tempo com o mesmo input. Ex: Dado um tema, escrever um poema E uma piada."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_core.runnables import RunnableParallel\n",
                "\n",
                "chain_poema = ChatPromptTemplate.from_template(\"Escreva um poema curto sobre {tema}.\") | llm | StrOutputParser()\n",
                "chain_piada = ChatPromptTemplate.from_template(\"Escreva uma piada sobre {tema}.\") | llm | StrOutputParser()\n",
                "\n",
                "mapa_combinado = RunnableParallel(poema=chain_poema, piada=chain_piada)\n",
                "\n",
                "resultado = mapa_combinado.invoke({\"tema\": \"Inteligência Artificial\"})\n",
                "\n",
                "print(\"=== POEMA ===\")\n",
                "print(resultado['poema'])\n",
                "print(\"\\n=== PIADA ===\")\n",
                "print(resultado['piada'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusão\n",
                "\n",
                "Vimos como compor chains sequencialmente e paralelamente.\n",
                "\n",
                "No próximo notebook, começaremos a construir nosso sistema RAG (Retrieval Augmented Generation), aprendendo a carregar e processar documentos."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}